import re
import logging
import requests
from bs4 import BeautifulSoup
import tldextract
import asyncio
import aiohttp
from datetime import datetime
import socket
import ssl
import whois

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def extract_domain(url):
    """Extract the domain and suffix from a given URL."""
    try:
        extracted = tldextract.extract(url)
        return extracted.domain, extracted.suffix
    except Exception as e:
        logging.error(f"Error extracting domain from URL: {url} - {e}")
        return None, None

def analyze_url_structure(url):
    """Analyze the URL structure and return a suspicion score based on its characteristics."""
    if len(url) > 100 or url.count('/') > 5 or url.count('.') > 3:
        return 20
    return 0

async def fetch_content(url):
    """Fetch the content of the URL asynchronously."""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                return await response.text()
    except Exception as e:
        logging.warning(f"Error fetching content from URL: {url} - {e}")
        return None

def analyze_content(content):
    """Analyze the content for suspicious elements like password fields or forms."""
    soup = BeautifulSoup(content, 'html.parser')
    if soup.find_all('input', {'type': 'password'}) or soup.find_all('form'):
        return 30
    return 0

def check_ssl_certificate(domain):
    """Check the SSL certificate of the domain."""
    try:
        context = ssl.create_default_context()
        with context.wrap_socket(socket.socket(), server_hostname=domain) as s:
            s.connect((domain, 443))
            cert = s.getpeercert()
            issuer = dict(x[0] for x in cert['issuer'])
            return 'Let\'s Encrypt' not in issuer.get('organizationName', '')
    except Exception as e:
        logging.warning(f"SSL certificate check failed for domain: {domain} - {e}")
        return True

def analyze_whois(domain):
    """Analyze WHOIS information to assess domain legitimacy."""
    try:
        domain_info = whois.whois(domain)
        creation_date = domain_info['creation_date']
        if isinstance(creation_date, list):
            creation_date = min(creation_date)
        if (datetime.now() - creation_date).days < 365:
            return 20
    except Exception as e:
        logging.warning(f"WHOIS analysis failed for domain: {domain} - {e}")
    return 0

def check_suspicious_keywords(domain, keywords):
    """Check if the domain contains any suspicious keywords."""
    return any(keyword in domain.lower() for keyword in keywords)

async def analyze_redirections(url):
    """Analyze redirections for the given URL."""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.head(url, allow_redirects=True) as response:
                if len(response.history) > 1:
                    return 25
                final_url = response.url
                logging.info(f"Final URL after redirection: {final_url}")
                return 0
    except Exception as e:
        logging.warning(f"Error analyzing redirections for URL: {url} - {e}")
        return 0

async def calculate_suspicion_score(url, keywords):
    """Calculate the suspicion score for the given URL."""
    if not re.match(r'https?://[^\s]+', url):
        logging.error(f"Invalid URL format: {url}")
        return 0

    domain, suffix = extract_domain(url)
    if not domain:
        return 0

    suspicion_score = 0
    suspicion_score += analyze_url_structure(url)

    content = await fetch_content(url)
    if content:
        suspicion_score += analyze_content(content)

    if check_ssl_certificate(domain):
        suspicion_score += 10

    suspicion_score += analyze_whois(domain)

    if '.' in domain:
        subdomain = domain.split('.')[0]
        if check_suspicious_keywords(subdomain, keywords):
            suspicion_score += 15

    suspicion_score += await analyze_redirections(url)

    if 'https://' not in url:
        suspicion_score += 15

    return max(0, min(100, suspicion_score))
